{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# define a transform to normalize the data\n",
    "# if the img has three channels, you should have three number for mean, \n",
    "# for example, img is RGB, mean is [0.5, 0.5, 0.5], the normalize result is R * 0.5, G * 0.5, B * 0.5. \n",
    "# If img is grey type that only one channel, mean should be [0.5], the normalize result is R * 0.5\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5], [0.5])\n",
    "                               ])\n",
    "# download and load the traning data\n",
    "trainset = datasets.MNIST('data/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# make an iterator for looping\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images[0].shape)\n",
    "# NOTE: The batch size is the number of images we get in one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to create a dense fully-connected network. \n",
    "\n",
    "Each unit in one layer is connected to the other in the next layer.\n",
    "The input to each layer must be one-dimensional vector. But our images are 28*28 2D tensors, so we need to convert them to 1D vectors. Therefore:\n",
    "* Convert/Flatten the batch of images of shape(64, 1, 28, 28) into (64, 28 * 28=784).\n",
    "* For the output layer, we also need 10 output units for the 10 classes(digits)\n",
    "* Also convert the network output into a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_images = images.view(64, 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n"
     ]
    }
   ],
   "source": [
    "print(flattened_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\"Create a sigmoid activation function.\n",
    "    Good for outputs that fall between 0 and 1. (probability)\n",
    "    args x: a torch tensor.\n",
    "    \"\"\"\n",
    "    return 1/(1 + torch.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Create a softmax activation function.\n",
    "    Good for outputs that fall between 0 and 1. (probability)\n",
    "    args x: a torch tensor.\n",
    "    \"\"\"\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "# flatten the images to shape(64, 784)\n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "# create parameters\n",
    "w1 = torch.randn(784, 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 10)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "h = activation(torch.mm(inputs, w1) + b1)\n",
    "\n",
    "out = torch.mm(h, w2) + b2\n",
    "probabilities = softmax(out)\n",
    "print(probabilities.shape)\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Torch nn to create networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"Use relu(Rectified linear unit) as the activation function.\n",
    "    Networks tend to train a lot faster when using relu.\n",
    "    For a network to approximate a non-linear function, the activation\n",
    "    function must be non-linear.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # inputs to hidden layer linear transformation\n",
    "        self.hidden_layer1 = nn.Linear(784, 128) # 256 outputs\n",
    "        self.hidden_layer2 = nn.Linear(128, 64)\n",
    "        # output layer, 10 units one for each digit\n",
    "        self.output = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # hidden layer with sigmoid activation\n",
    "        x = F.relu(self.hidden_layer1(x))\n",
    "        x = F.relu(self.hidden_layer2(x))\n",
    "        # Output layer with softmax activation\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden_layer1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (hidden_layer2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3058, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10)\n",
    "                     )\n",
    "\n",
    "# define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# forward pass, get the logits\n",
    "logits = model(images)\n",
    "# calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more convenient to build model with a log-softmax output using `nn.LogSoftmax`\n",
    "We can get actual probabilities by taking the exponential torch.exp(output).\n",
    "We'll also use the negative log likelihood loss, `nn.NLLLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3025, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1),\n",
    "                     )\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "logits = model(images)\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
